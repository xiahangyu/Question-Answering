{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Build BM25 models for each document.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenization(sent):\n",
    "    tokenized_sent = word_tokenize(sent)\n",
    "    return [word for word in tokenized_sent if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"./data/training.json\", encoding='utf-8')\n",
    "training = json.load(f)\n",
    "train_qs = [tokenization(item['question']) for item in training]\n",
    "train_texts = [tokenization(item['text']) for item in training]\n",
    "train_aps = [item['answer_paragraph'] for item in training]\n",
    "train_docids = [item['docid'] for item in training]\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open(\"./data/testing.json\", encoding='utf-8')\n",
    "testing = json.load(f)\n",
    "test_qs = [tokenization(item['question']) for item in testing]\n",
    "test_docids = [item['docid'] for item in testing]\n",
    "f.close()\n",
    "\n",
    "f = open(\"./data/documents.json\", encoding='utf-8')\n",
    "documents = json.load(f)\n",
    "docs = [ [tokenization(para) for para in doc['text']] for doc in documents]\n",
    "docids = [doc['docid'] for doc in documents]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = [bm25.BM25(corpus) for corpus in docs]\n",
    "average_idf = [sum(map(lambda k: float(bm25Model[i].idf[k]), bm25Model[i].idf.keys())) / len(bm25Model[i].idf.keys()) for i in range(len(bm25Model))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npred_aps = []\\nfor i in range(len(train_qs)):\\n    docid = train_docids[i]\\n    question = train_qs[i]\\n    scores = bm25Model[docid].get_scores(question, average_idf[docid])\\n    pred_aps.append(scores.index(max(scores)))\\n    \\nprint(np.sum(np.array(pred_aps)==np.array(train_aps))/len(train_aps))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pred_aps = []\n",
    "for i in range(len(train_qs)):\n",
    "    docid = train_docids[i]\n",
    "    question = train_qs[i]\n",
    "    scores = bm25Model[docid].get_scores(question, average_idf[docid])\n",
    "    pred_aps.append(scores.index(max(scores)))\n",
    "    \n",
    "print(np.sum(np.array(pred_aps)==np.array(train_aps))/len(train_aps))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Find answer paragraphs for all questions in training and testing set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ap_text = []\n",
    "for i in range(len(train_qs)):\n",
    "    docid = train_docids[i]\n",
    "    ap = train_aps[i]\n",
    "    train_ap_text.append(docs[docid][ap])\n",
    "\n",
    "test_ap_text = []\n",
    "for i in range(len(test_qs)):\n",
    "    docid = test_docids[i]\n",
    "    question = test_qs[i]\n",
    "    scores = bm25Model[docid].get_scores(question, average_idf[docid])\n",
    "    ap = scores.index(max(scores))\n",
    "    test_ap_text.append(docs[docid][ap])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train a NN to predict the answer to a question after a paragraph is given.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def validWord(word):\n",
    "    return re.match(\"^[a-zA-z]*$\", word) != None\n",
    "\n",
    "vocab = set()\n",
    "for doc in docs:\n",
    "    for para in doc:\n",
    "        vocab |= set(para)\n",
    "for q in train_qs:\n",
    "    vocab |= set(q)\n",
    "for text in train_texts:\n",
    "    vocab |= set(text)\n",
    "vocab = set([word for word in vocab if validWord(word)])\n",
    "vocab |= set([\"\\t\",\"\\n\"])\n",
    "    \n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "word_index = dict([(word, i) for i, word in enumerate(vocab)])\n",
    "reverse_word_index = dict((i,word) for word,i in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index.get(\"123\")==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "'''\n",
    "embedding_size = 500\n",
    "paras = [para for doc in docs for para in doc] + train_qs + train_texts\n",
    "embedding_model = Word2Vec(paras, size = embedding_size, min_count = 0)\n",
    "\n",
    "vocab = dict([(word,i) for i,word in enumerate(list(embedding_model.wv.vocab))])\n",
    "reverse_vocab = dict((i,word) for word,i in vocab.items())\n",
    "vocab_size = len(vocab)\n",
    "'''\n",
    "\n",
    "#merge answer paragraph with question. They are seperated by a word \"Q\"\n",
    "train_xs = [ para+question for i, (para,question) in enumerate(zip(train_ap_text, train_qs))]\n",
    "train_ys = [ [\"\\t\"]+answer+[\"\\n\"] for answer in train_texts]\n",
    "\n",
    "#input_size = len(train_xs)\n",
    "max_encoder_seq_length = max([len(x) for x in train_xs])\n",
    "max_decoder_seq_length = max([len(y) for y in train_ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a890e3f33b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m encoder_input_data = np.zeros(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_encoder_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     dtype='float32')\n\u001b[1;32m      4\u001b[0m decoder_input_data = np.zeros(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_decoder_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (input_size, max_encoder_seq_length, vocab_size),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (input_size, max_decoder_seq_length, vocab_size),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (input_size, max_decoder_seq_length, vocab_size),\n",
    "    dtype='float32')\n",
    "\n",
    "for i,x in enumerate(train_xs):\n",
    "    if i < input_size:\n",
    "        for j,word in enumerate(x):\n",
    "            encoder_input_data[i,j, word_index[word]] = 1.\n",
    "        \n",
    "for i,y in enumerate(train_ys):\n",
    "    if i < input_size:\n",
    "        for j,word in enumerate(y):\n",
    "            decoder_input_data[i,j, word_index[word]] = 1.\n",
    "            if j > 0 :\n",
    "                decoder_target_data[i, j-1, word_index[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "'''Train a lstm_seq2seq model.\n",
    "        Code reused from https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, vocab_size))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, vocab_size))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim,\n",
    "                    return_sequences=True,\n",
    "                    return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n",
      "12232\n",
      "Stop\n"
     ]
    }
   ],
   "source": [
    "def answer(para_question):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(para_question)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, vocab_size))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, word_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    answer = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        print(token_index)\n",
    "        word = reverse_word_index[token_index]\n",
    "        print(word)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (word == '\\n' or len(answer) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            answer.append(word)\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, vocab_size))\n",
    "        target_seq[0, 0, word_index[word]] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return answer\n",
    "\n",
    "for i in range(400,420):\n",
    "    #embedding_x = np.array([[embedding_model[word] for word in train_xs[i]]])\n",
    "    my_answer = answer(encoder_input_data[i:i+1])\n",
    "    if i < 20:\n",
    "        #print('-')\n",
    "        #print('  Text:', train_ap_text[i])\n",
    "        #print(\"  Question:\", train_qs[i])\n",
    "        print('  Answer:', train_texts[i], \"     My answer:\", my_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
