{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Build BM25 models for each document.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenization(sent):\n",
    "    return word_tokenize(sent)\n",
    "    #tokenized_sent = word_tokenize(sent)\n",
    "    #return [word for word in tokenized_sent if word.lower() not in stop_words]\n",
    "\n",
    "def removeStopw(tokenized_sent):\n",
    "    return [word for word in tokenized_sent if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"./data/training.json\", encoding='utf-8')\n",
    "training = json.load(f)\n",
    "train_qs = [tokenization(item['question']) for item in training]\n",
    "train_texts = [tokenization(item['text']) for item in training]\n",
    "train_aps = [item['answer_paragraph'] for item in training]\n",
    "train_docids = [item['docid'] for item in training]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/testing.json\", encoding='utf-8')\n",
    "testing = json.load(f)\n",
    "test_qs = [tokenization(item['question']) for item in testing]\n",
    "test_docids = [item['docid'] for item in testing]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/documents.json\", encoding='utf-8')\n",
    "documents = json.load(f)\n",
    "docs = [ [tokenization(para) for para in doc['text']] for doc in documents]\n",
    "docids = [doc['docid'] for doc in documents]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import bm25\n",
    "bm25Model = [bm25.BM25(corpus) for corpus in docs]\n",
    "average_idf = [sum(map(lambda k: float(bm25Model[i].idf[k]), bm25Model[i].idf.keys())) / len(bm25Model[i].idf.keys()) for i in range(len(bm25Model))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Find answer paragraphs for all questions in training and testing set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ap_text = []\n",
    "for i in range(len(train_qs)):\n",
    "    docid = train_docids[i]\n",
    "    ap = train_aps[i]\n",
    "    train_ap_text.append(docs[docid][ap])\n",
    "\n",
    "test_ap_text = []\n",
    "for i in range(len(test_qs)):\n",
    "    docid = test_docids[i]\n",
    "    question = test_qs[i]\n",
    "    scores = bm25Model[docid].get_scores(question, average_idf[docid])\n",
    "    ap = scores.index(max(scores))\n",
    "    test_ap_text.append(docs[docid][ap])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train a NN to predict the answer to a question after a paragraph is given.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 500\n",
    "paras = [para for doc in docs for para in doc] + train_qs + train_texts\n",
    "paras.append([\"\\t\",\"\\n\"])\n",
    "embedding_model = Word2Vec(paras, size = embedding_size, min_count = 0)\n",
    "\n",
    "vocab = dict([(word,i) for i,word in enumerate(list(embedding_model.wv.vocab))])\n",
    "reverse_vocab = dict([(i,word) for word,i in vocab.items()])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "#merge answer paragraph with question. They are seperated by a word \"Q\"\n",
    "train_xs = [ para+[\"\\t\"]+question for i, (para,question) in  enumerate(zip(train_ap_text, train_qs))]\n",
    "train_ys = [ [\"\\t\"]+answer+[\"\\n\"] for answer in train_texts]\n",
    "test_xs = [ para+[\"\\t\"]+question for i, (para,question) in  enumerate(zip(test_ap_text, test_qs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "input_size = len(train_xs)\n",
    "max_encoder_seq_length = max([len(x) for x in train_xs])\n",
    "max_decoder_seq_length = max([len(y) for y in train_ys])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (input_size, max_encoder_seq_length, embedding_size),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (input_size, max_decoder_seq_length, embedding_size),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (input_size, max_decoder_seq_length, vocab_size),\n",
    "    dtype='float32')\n",
    "\n",
    "for i,x in enumerate(train_xs):\n",
    "    if i < input_size:\n",
    "        for j,word in enumerate(x):\n",
    "            encoder_input_data[i,j] = embedding_model[word]\n",
    "        \n",
    "for i,y in enumerate(train_ys):\n",
    "    if i < input_size:\n",
    "        for j,word in enumerate(y):\n",
    "            decoder_input_data[i,j] = embedding_model[word]\n",
    "            if j > 0 :\n",
    "                decoder_target_data[i, j-1, vocab[word] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/7\n",
      "240/240 [==============================] - 23s 97ms/step - loss: 7.0405 - val_loss: 0.4461\n",
      "Epoch 2/7\n",
      "240/240 [==============================] - 19s 77ms/step - loss: 0.5704 - val_loss: -8.2318\n",
      "Epoch 3/7\n",
      "240/240 [==============================] - 17s 72ms/step - loss: -11.0784 - val_loss: 10.0092\n",
      "Epoch 4/7\n",
      "240/240 [==============================] - 17s 71ms/step - loss: 13.7186 - val_loss: -4.6421\n",
      "Epoch 5/7\n",
      "240/240 [==============================] - 19s 80ms/step - loss: -0.4053 - val_loss: -12.8917\n",
      "Epoch 6/7\n",
      "240/240 [==============================] - 18s 73ms/step - loss: -12.7759 - val_loss: -21.9524\n",
      "Epoch 7/7\n",
      "240/240 [==============================] - 18s 77ms/step - loss: -7.7733 - val_loss: 17.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer lstm_16 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_15/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_15/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "'''Train a lstm_seq2seq model.\n",
    "        Code reused from https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, embedding_size))\n",
    "encoder = LSTM(latent_dim, return_state=True, dropout=0.2)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, embedding_size))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim,\n",
    "                    return_sequences=True,\n",
    "                    return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(para_question):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(para_question)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, embedding_size))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = embedding_model['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    answer = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        word = reverse_vocab[token_index]\n",
    "    \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (word == '\\n' or len(answer) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            answer.append(word)\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, embedding_size))\n",
    "        target_seq[0, 0] = embedding_model[word]\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-852539769634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0membedding_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_xs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmy_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('result.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"id\",\"answer\"])\n",
    "    for i in range(len(test_xs)):\n",
    "        embedding_x = np.array([[embedding_model[word] for word in train_xs[i]]])\n",
    "        my_answer = answer(np.array(embedding_x))\n",
    "        writer.writerow([str(i), \" \".join(my_answer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
